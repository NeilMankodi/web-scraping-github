{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09428468",
   "metadata": {},
   "source": [
    "# Web Scraping Github Topics Using Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8e67e1",
   "metadata": {},
   "source": [
    "### By Neil Mankodi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4639c4a",
   "metadata": {},
   "source": [
    "### Steps covered in this project:\n",
    "- Identify site to scrape\n",
    "    - In this project we will be scraping \"https://github.com/topics\"\n",
    "- Goal of this project\n",
    "    - Scrape the github web pages to extract information about the topics listed on the above mentioned url and then for each topic, scrape the topic specific url to extract information about the top repositories of that topic\n",
    "    - Save all this information in csv files which can then be used for future analysis\n",
    "- Install required libraries. Libraries used in this project are as follows:\n",
    "    - Requests\n",
    "    - BeautifulSoup\n",
    "    - Pandas\n",
    "    - OS\n",
    "- Use requests library to download web page \"https://github.com/topics\"\n",
    "- Use the BeautifulSoup library to parse and extract the following information regarding the topics listed on the web page:\n",
    "    - Title of the topic\n",
    "    - Description of the topic\n",
    "    - URL to the topic's web page\n",
    "- For each topic listed on the above web page:\n",
    "    - use the requests library to download the specific web page eg. \"https://github.com/topics/3D\"\n",
    "    - use the BeautifulSoup library to parse and extract information from the web page\n",
    "    - scrape the web page of that topic to extract the following information for all the repositories listed\n",
    "        - Username of owner\n",
    "        - Name of the repository\n",
    "        - Number of stars achieved by that repository\n",
    "        - URL for the repository\n",
    "- Save all extracted information as seperate csv files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329f9b68",
   "metadata": {},
   "source": [
    "#### Importing all required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b7ec984",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e35673",
   "metadata": {},
   "source": [
    "#### Define functions to scrape the seed url (\"https://github.com/topics\")\n",
    "- get_doc(topic_url) function takes the topic url as a parameter, uses the requests library to download the web page, parses the downloaded web page using BeautifulSoup and returns the parsed doc\n",
    "- scrape_topics(topic_url) function takes the topic url as a parameter, extracts the required information from the html of the document and returns the information in the form of lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9b2184d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_doc(topic_url):\n",
    "    # use request library to get the page\n",
    "    response = requests.get(topic_url)\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        raise Exception(\"Failed to load page {}\".format(topic_url))\n",
    "    \n",
    "    # parse the page contents using beautiful soup\n",
    "    doc = BeautifulSoup(response.text, 'html.parser')\n",
    "    return doc\n",
    "\n",
    "def scrape_topics(topic_url):\n",
    "    doc = get_doc(topic_url)\n",
    "\n",
    "    selection_class_topics = \"f3 lh-condensed mb-0 mt-1 Link--primary\"\n",
    "    topic_title_tags = doc.find_all('p', {'class': selection_class_topics})\n",
    "\n",
    "    selection_class_topic_desc = \"f5 color-fg-muted mb-0 mt-1\"\n",
    "    topic_desc_tags = doc.find_all('p', class_ = selection_class_topic_desc)\n",
    "\n",
    "    topic_link_tags = []\n",
    "    for topic_title_tag in topic_title_tags:\n",
    "        a_tag = topic_title_tag.parent\n",
    "        topic_link_tags.append(a_tag)\n",
    "\n",
    "    topic_titles = []\n",
    "    topic_descs = []\n",
    "    topic_urls = []\n",
    "\n",
    "    for tag in range(len(topic_title_tags)):\n",
    "        topic_titles.append(topic_title_tags[tag].text)\n",
    "        topic_descs.append(topic_desc_tags[tag].text.strip())\n",
    "        topic_urls.append(\"https://github.com\" + topic_link_tags[tag]['href'])\n",
    "\n",
    "    return topic_titles, topic_descs, topic_urls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9fde8b",
   "metadata": {},
   "source": [
    "#### Define functions to scrape the topic url (eg. \"https://github.com/topics/3D\")\n",
    "- parse_star_count(stars_str) is a helper function that takes the star count string that has been extracted from the web page for each repo and returns the integer equivalent\n",
    "- get_repo_info(repo_tag, star_tag) is a helper function that takes the extracted tags that store the required information, extracts the required information and returns this information\n",
    "- get_topic_repos(topic_url) is a function that takes the topic url as a parameter, extracts required information for each repository with the help of the helper functions, creates a pandas dataframe from the information and returns the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad9be838",
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions to get DataFrame for a particular topic\n",
    "\n",
    "def parse_star_count(stars_str):\n",
    "    stars_str = stars_str.strip()\n",
    "    if ',' in stars_str:\n",
    "        stars_str = stars_str.replace(',', '')\n",
    "    stars_str = int(stars_str)\n",
    "    return(stars_str)\n",
    "\n",
    "def get_repo_info(repo_tag, star_tag):\n",
    "    a_tags = repo_tag.find_all('a')\n",
    "    username = a_tags[0].text.strip()\n",
    "    repo_name = a_tags[1].text.strip()\n",
    "    repo_url = \"https://github.com\" + a_tags[1]['href']\n",
    "    stars = parse_star_count(star_tag['title'])\n",
    "    return username, repo_name, stars, repo_url\n",
    "\n",
    "def get_topic_repos(topic_url):\n",
    "    topic_doc = get_doc(topic_url)\n",
    "\n",
    "    # get the parent tag (h3) which has the required tags\n",
    "    h3_selector = \"f3 color-fg-muted text-normal lh-condensed\"\n",
    "    repo_tags = topic_doc.find_all('h3', class_=h3_selector)\n",
    "\n",
    "    # get the tag that contains stars info\n",
    "    stars_selector = \"Counter js-social-count\"\n",
    "    star_tags = topic_doc.find_all('span', class_=stars_selector)\n",
    "\n",
    "    # empty dict that will later be used to create the dataframe\n",
    "    topic_repos_dict = {\n",
    "        'username': [],\n",
    "        'repo_name': [],\n",
    "        'stars': [],\n",
    "        'repo_url': []\n",
    "    }\n",
    "\n",
    "    for i in range(len(repo_tags)):\n",
    "        repo_info = get_repo_info(repo_tags[i], star_tags[i])\n",
    "        topic_repos_dict['username'].append(repo_info[0])\n",
    "        topic_repos_dict['repo_name'].append(repo_info[1])\n",
    "        topic_repos_dict['stars'].append(repo_info[2])\n",
    "        topic_repos_dict['repo_url'].append(repo_info[3])\n",
    "\n",
    "    topic_repos_df = pd.DataFrame(topic_repos_dict)\n",
    "\n",
    "    return topic_repos_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0998029",
   "metadata": {},
   "source": [
    "#### Define the main function that will be called to initiate the scraping process\n",
    "- scrape_topics_repos() is a function that creates a list of dataframes containing required information. These dataframes are obtained with the help of the functions defined in the previous sections. This function then returns the list of all dataframes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e9e8f0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main function\n",
    "\n",
    "def scrape_topics_repos():\n",
    "    url = \"https://github.com/topics\"\n",
    "    topic_details = scrape_topics(url)\n",
    "\n",
    "    # topic_titles = topic_details[0]\n",
    "    # topic_descs = topic_details[1]\n",
    "    # topic_urls = topic_details[2]\n",
    "\n",
    "    topic_dict = {\n",
    "        'title': topic_details[0],\n",
    "        'description': topic_details[1],\n",
    "        'url': topic_details[2]\n",
    "    }\n",
    "\n",
    "    topic_df = pd.DataFrame(topic_dict)\n",
    "    # for each topic we need to get the dataframe of info\n",
    "    # first df will be that of list of topics\n",
    "    all_df = [topic_df]\n",
    "\n",
    "    for url in topic_dict['url']:\n",
    "        df = get_topic_repos(url)\n",
    "        all_df.append(df)\n",
    "\n",
    "    return all_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4e9f45d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df = scrape_topics_repos()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1426541",
   "metadata": {},
   "source": [
    "#### Save the extracted information as csv files\n",
    "- we first create a directory which will store all the csv files. This is done to improve usabilty and organization of the project directory. We use the OS library to perform this function.\n",
    "- finally we iterate over the list of dataframes created in the previous sections and create a csv for each dataframe. These files are stored in the directory we created in the previous step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "76f6adc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"scraped data\", exist_ok=True)\n",
    "\n",
    "topic_titles = all_df[0]['title']\n",
    "\n",
    "for i in range(len(all_df)):\n",
    "    if i == 0:\n",
    "        all_df[i].to_csv(\"scraped data/{}\".format(\"allTopics.csv\"), index = None)\n",
    "    else:\n",
    "        all_df[i].to_csv(\"scraped data/{}\".format(topic_titles[i-1] + \".csv\"), index = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11937e8f",
   "metadata": {},
   "source": [
    "#### End Notes\n",
    "We have successfully accomplished the goals for this project that were defined at the beginning. We have extracted the required information from the web pages and stored this information in csv files. These csv files can later be used to conduct all sorts of data analysis projects.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
